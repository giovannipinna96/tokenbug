#!/usr/bin/env python3
"""
Process SQL Bugs Dataset

This script processes the output from generate_buggy_sql.py to create a structured
dataset for training SQL bug detection models. It:
1. Reads buggy SQL queries generated by LLM
2. Computes diffs between buggy and correct queries
3. Identifies which lines are buggy
4. Adds correct queries as "stable" examples
5. Splits data into train/validation/test sets

Based on process_pytracebugs.py from the Python bug detection pipeline.

Usage:
    python process_sql_bugs.py \
        --input-json data/buggy_sql_output.json \
        --output-json data/processed_sql_dataset.json \
        --train-ratio 0.7 \
        --val-ratio 0.15 \
        --test-ratio 0.15
"""

import argparse
import json
import sys
from pathlib import Path
from typing import List, Dict, Any
from tqdm import tqdm

from sql_utils import (
    infer_buggy_sql_lines_from_diff,
    extract_buggy_lines_text,
    get_sql_query_type,
    normalize_sql_query,
    split_dataset_by_ratio,
)


def process_buggy_sql_entry(
    entry: Dict[str, Any],
    split: str
) -> Dict[str, Any]:
    """
    Process a single buggy SQL entry from generate_buggy_sql.py output.

    Args:
        entry: Dictionary with buggy_query, correct_query, etc.
        split: 'train', 'validation', or 'test'

    Returns:
        Processed entry with buggy_lines_indices and metadata
    """
    # Extract fields
    correct_query = entry['correct_query']
    buggy_query = entry['buggy_query']
    user_request = entry.get('user_request', '')
    table_name = entry.get('table_name', '')
    database = entry.get('database', '')
    error_type = entry.get('error_type', 'unknown')

    # Infer buggy lines using diff
    buggy_lines_indices = infer_buggy_sql_lines_from_diff(
        buggy_query,
        correct_query,
        normalize=True
    )

    # Extract buggy line texts
    buggy_lines_text = extract_buggy_lines_text(buggy_query, buggy_lines_indices)

    # Infer query type
    query_type = get_sql_query_type(correct_query)

    # Create processed entry
    processed = {
        'correct_query': correct_query,
        'buggy_query': buggy_query,
        'buggy_lines_indices': buggy_lines_indices,
        'buggy_lines_text': buggy_lines_text,
        'dataset_type': 'buggy',
        'split': split,
        'query_type': query_type,
        'error_type': error_type,
        'user_request': user_request,
        'table_name': table_name,
        'database': database,
    }

    return processed


def create_stable_entry(
    entry: Dict[str, Any],
    split: str
) -> Dict[str, Any]:
    """
    Create a "stable" (correct) entry from a buggy entry's correct query.

    Args:
        entry: Original buggy entry
        split: 'train', 'validation', or 'test'

    Returns:
        Stable entry with no buggy lines
    """
    correct_query = entry['correct_query']

    stable = {
        'correct_query': correct_query,
        'buggy_query': correct_query,  # Same as correct
        'buggy_lines_indices': [],  # No buggy lines
        'buggy_lines_text': [],
        'dataset_type': 'stable',
        'split': split,
        'query_type': entry.get('query_type', get_sql_query_type(correct_query)),
        'error_type': 'none',
        'user_request': entry.get('user_request', ''),
        'table_name': entry.get('table_name', ''),
        'database': entry.get('database', ''),
    }

    return stable


def process_sql_bugs_dataset(
    input_data: List[Dict[str, Any]],
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    include_stable: bool = True
) -> List[Dict[str, Any]]:
    """
    Process entire SQL bugs dataset.

    Args:
        input_data: List of entries from generate_buggy_sql.py
        train_ratio: Proportion for training
        val_ratio: Proportion for validation
        test_ratio: Proportion for testing
        include_stable: Whether to include stable (correct) examples

    Returns:
        List of processed entries
    """
    total_entries = len(input_data)
    splits = split_dataset_by_ratio(total_entries, train_ratio, val_ratio, test_ratio)

    processed_entries = []

    print(f"Processing {total_entries} buggy SQL entries...")
    print(f"Split ratios - Train: {train_ratio}, Val: {val_ratio}, Test: {test_ratio}")

    # Process buggy entries
    for idx, entry in enumerate(tqdm(input_data, desc="Processing buggy entries")):
        # Determine split
        if idx in splits['train']:
            split = 'train'
        elif idx in splits['validation']:
            split = 'validation'
        else:
            split = 'test'

        # Process buggy entry
        try:
            processed = process_buggy_sql_entry(entry, split)
            processed_entries.append(processed)

            # Optionally add stable entry
            if include_stable:
                stable = create_stable_entry(entry, split)
                processed_entries.append(stable)

        except Exception as e:
            print(f"\nWarning: Failed to process entry {idx}: {e}")
            continue

    # Print statistics
    print(f"\nProcessed {len(processed_entries)} total entries:")

    split_counts = {'train': 0, 'validation': 0, 'test': 0}
    type_counts = {'buggy': 0, 'stable': 0}

    for entry in processed_entries:
        split_counts[entry['split']] += 1
        type_counts[entry['dataset_type']] += 1

    print(f"  Train: {split_counts['train']}")
    print(f"  Validation: {split_counts['validation']}")
    print(f"  Test: {split_counts['test']}")
    print(f"  Buggy: {type_counts['buggy']}")
    print(f"  Stable: {type_counts['stable']}")

    # Count lines with bugs
    total_buggy_lines = sum(
        len(e['buggy_lines_indices'])
        for e in processed_entries
        if e['dataset_type'] == 'buggy'
    )
    print(f"  Total buggy lines identified: {total_buggy_lines}")

    return processed_entries


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Process SQL bugs dataset from generate_buggy_sql.py output",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage
  python process_sql_bugs.py \
    --input-json data/buggy_sql_output.json \
    --output-json data/processed_sql_dataset.json

  # Custom split ratios
  python process_sql_bugs.py \
    --input-json data/buggy_sql_output.json \
    --output-json data/processed_sql_dataset.json \
    --train-ratio 0.8 \
    --val-ratio 0.1 \
    --test-ratio 0.1

  # Without stable examples
  python process_sql_bugs.py \
    --input-json data/buggy_sql_output.json \
    --no-stable
        """
    )

    parser.add_argument(
        '--input-json',
        type=str,
        required=True,
        help='Path to input JSON from generate_buggy_sql.py'
    )

    parser.add_argument(
        '--output-json',
        type=str,
        default='data/processed_sql_dataset.json',
        help='Path to output JSON (default: data/processed_sql_dataset.json)'
    )

    parser.add_argument(
        '--train-ratio',
        type=float,
        default=0.7,
        help='Training set ratio (default: 0.7)'
    )

    parser.add_argument(
        '--val-ratio',
        type=float,
        default=0.15,
        help='Validation set ratio (default: 0.15)'
    )

    parser.add_argument(
        '--test-ratio',
        type=float,
        default=0.15,
        help='Test set ratio (default: 0.15)'
    )

    parser.add_argument(
        '--no-stable',
        action='store_true',
        help='Do not include stable (correct) examples'
    )

    return parser.parse_args()


def main():
    """Main entry point."""
    args = parse_args()

    # Validate ratios
    total_ratio = args.train_ratio + args.val_ratio + args.test_ratio
    if abs(total_ratio - 1.0) > 1e-6:
        print(f"Error: Ratios must sum to 1.0 (got {total_ratio})")
        sys.exit(1)

    # Load input data
    input_path = Path(args.input_json)
    if not input_path.exists():
        print(f"Error: Input file not found: {input_path}")
        sys.exit(1)

    print(f"Loading data from: {input_path}")
    with open(input_path, 'r') as f:
        input_data = json.load(f)

    if not isinstance(input_data, list):
        print("Error: Input JSON must be a list of entries")
        sys.exit(1)

    print(f"Loaded {len(input_data)} entries\n")

    # Process dataset
    processed = process_sql_bugs_dataset(
        input_data,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        include_stable=not args.no_stable
    )

    # Save output
    output_path = Path(args.output_json)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    print(f"\nSaving processed dataset to: {output_path}")
    with open(output_path, 'w') as f:
        json.dump(processed, f, indent=2)

    print(f"Done! Processed dataset saved with {len(processed)} entries.")

    # Print example
    if processed:
        print("\nExample processed entry:")
        example = processed[0]
        print(f"  Query type: {example['query_type']}")
        print(f"  Dataset type: {example['dataset_type']}")
        print(f"  Split: {example['split']}")
        print(f"  Buggy lines: {example['buggy_lines_indices']}")
        print(f"  Correct query: {example['correct_query'][:80]}...")
        print(f"  Buggy query: {example['buggy_query'][:80]}...")


if __name__ == "__main__":
    main()
